% !TEX root = ../thesis.tex

\chapter{相关工作}
大数据的发展和基于托管式语言的平台开发带动了Hadoop生态圈的成功。目前Hadoop生态圈共有MapReduce，Tez，Spark及Flink等分布式计算引擎，
分布式计算引擎项目之间的竞争也相当激烈。随着各个项目的发展与日益成熟，通过改进分布式计算框架本身大幅提高性能的机会越来越少。
同时，在当前数据中心的硬件配置中，采用了越来越多更先进的IO设备，例如SSD存储，10G甚至是40Gbps网络，IO带宽的提升非常明显，
许多计算密集类型的工作负载的瓶颈已经取决于底层硬件系统的吞吐量，而不是传统上人们认为的IO带宽，而CPU和内存的利用效率，
则很大程度上决定了底层硬件系统的吞吐量。所以越来越多的项目将眼光投向了高级语言虚拟机本身，希望通过解决虚拟机本身带来的一些问题，
提高分布式系统的性能或是健壮性。大数据场景下，GC常常成为性能瓶颈，节点见的数据传输会带来巨大的性能开销，除此之外，多节点各自独立工作不合作也会带来重复劳动和性能损失。

\section{GC}
托管式语言提供了自动内存管理的特性，自动内存管理是通过GC实现的。在托管式语言实现的大数据系统中，
所有的数据都是存储在堆上的对象，对象的分配与回收都是由内存管理模块实现的，区别于非托管式语言像C或C++需要手动去malloc和free。
这固然减轻了开发者的工作压力，但是也带来了额外的性能开销。尤其是在大数据场景中，内存管理模块的对象分配与回收策略在最初设计时
并没有考虑到大数据的特征：数据量巨大并且大量数据具有相似的生命周期。这导致内存管理开销巨大，占据了大数据系统50\% 以上的执行时间，
极大的损害了性能，并且这种开销是不可以通过扩展的方式来解决。

考虑到数据对象数量巨大给内存管理带来的压力，FACADE\cite{nguyen2015facade}在编译层限制数据对象的数量，以此降低内存管理开销。FACADE编译大数据应用源码，
生成数据管理高效的代码。在生成的代码中，每个线程在堆中创建的对象的数量是有上限的，因此整个堆中数据对象相较于之前大大减少，
内存管理开销也相应降低。FACADE减少了大数据系统3\%-48\%的执行时间，将GC次数降低了88倍，内存消耗降低了50\%，但是编译前需要开发者在代码中标记数据对象，使用成本较高。

很多学者使用基于region的内存管理来降低GC开销。考虑到大数据中大量数据对象具有相似生命周期，Broom\cite{gog2015broom}抛弃GC，使用大小不同的region来管理具有相似生命周期的数据对象，
将大数据系统执行开销降低了34\%，但是需要开发者在代码中标记数据对象的生命周期，使用成本较高。Yak\cite{nguyen2016yak}是一个JVM的垃圾收集器，针对大数据中数据对象生命周期的epoch特性，将堆空间划分为一个个region，
每个region对应一个epoch，相同epoch的对象被分配到同一个region中，回收时也同时被回收。与JVM默认GC收集器 Parallel Scavenge相比，Yak将GC时延降低了1.4-44.3倍，Yak不需要对代码做很久修改，对开发者较为友好。

\section{数据传输}
在大数据系统中，一个很常见的任务是在集群里面的多个工作节点之间传输数据。例如在Hadoop上做一个MapReduce任务，在做reduce时，数据会从所有的map节点传输到reduce节点。
由于大数据系统是由托管式语言像Java或Scala实现的，所有的数据是以对象形式存储的。因此大数据系统中，数据从一个源节点传输到另一个目标节点需要经历三个步骤：
\begin{enumerate}
    \item 源节点将数据对象序列化为字节序列
    \item 源节点将序列化后的字节序列发送给目标节点
    \item 目标节点接收字节序列反序列化为数据对象
\end{enumerate}
尽管前人已经在序列化和反序列化方向上做了很多优化，但序列化与反序列化仍然是一个巨大的性能开销，占据了Spark 30\%的执行时间。因此Nguyen实现了Skyway\cite{nguyen2018skyway}，
一种基于JVM的技术，直接在不同节点的堆上传输对象，避免了原来节点间传输数据需要序列化再反序列化所带来的巨大性能开销，开发者也因此不需要去手写序列化与反序列化函数。
实验表明，在Spark与Flink系统上，Skyway的性能要比Java序列化标准集（JSBS）中所有的Java序列化库（一共90个）都要好。Skyway使得Spark的性能提神了16\%～36\%，Flink提升了19\%。

\section{分布式}
分布式是让大数据任务更快速运行的一个重要方法。通过将数据集切割成多个子数据集，并在不同的节点上运行相同的任务处理子数据集可以大大提高数据处理的速度。
但是分布式也带来了其他问题，多节点各自独立工作不合作带来的重复劳动和性能损失。Lion和Chiu\cite{lion2016don}发现JVM多节点运行时，每个节点都需要warm-up
（加载类和解释字节码等），重复劳动并且带来了额外的性能开销，降低了可扩展性。所以他们实现了HotTub，通过在多个节点之间重用已经热身的JVM池，
消除了分布式大数据系统中每个节点都要Warm-up所带来的开销，提高了可扩展性。Maas\cite{maas2016taurus, maas2015trash}发现在延迟敏感的大数据系统中，一个节点的GC延迟
可能导致多个其他节点的等待。所以他实现了Taurus，一个包含所有节点运行时的整体性的运行时，用来协调多个节点做出配合的决定，例如GC。










